{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T16:39:36.206093Z",
     "start_time": "2023-07-11T16:39:34.400639Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir('prepared') if f.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('sentiment'):\n",
    "    os.makedirs('sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('log'):\n",
    "    os.makedirs('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sentiment analysis libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_vader(text):\n",
    "    global counter\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 1000 == 0:\n",
    "        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        logging.info(f\"vader:{timestamp}: {counter}/{total}, {counter/total*100}%\")\n",
    "    \n",
    "    score = analyzer.polarity_scores(text)\n",
    "    return score['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_pattern(text):\n",
    "    global counter\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 1000 == 0:\n",
    "        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        logging.info(f\"pattern:{timestamp}: {counter}/{total}, {counter/total*100}%\")\n",
    "    \n",
    "    return sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Select GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "from scipy.special import softmax\n",
    "\n",
    "model_src = 'cardiffnlp/twitter-xlm-roberta-base-sentiment'\n",
    "\n",
    "nlp = AutoModelForSequenceClassification.from_pretrained(model_src).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_src)\n",
    "config = AutoConfig.from_pretrained(model_src)\n",
    "\n",
    "tokenizer.save_pretrained(model_src)\n",
    "nlp.save_pretrained(model_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_xlm(text):    \n",
    "    global counter\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 1000 == 0:\n",
    "        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        logging.info(f\"xlm-t:{timestamp}: {counter}/{total}, {counter/total*100}%\")\n",
    "    \n",
    "    if len(text) > 512: text = text[:512]\n",
    "    \n",
    "    token = tokenizer(text, return_tensors='pt').to(device)\n",
    "    output = nlp(**token)\n",
    "    scores = softmax(output[0][0].detach().cpu().numpy())\n",
    "\n",
    "    return (scores[0] * -1) + scores[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating: MSFT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30137/30137 [02:05<00:00, 240.39it/s]\n",
      "100%|██████████| 30137/30137 [01:56<00:00, 257.70it/s]\n",
      "100%|██████████| 30137/30137 [07:17<00:00, 68.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating: AAPL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39306/39306 [02:48<00:00, 233.20it/s]\n",
      "100%|██████████| 39306/39306 [02:45<00:00, 237.36it/s]\n",
      "100%|██████████| 39306/39306 [09:37<00:00, 68.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating: MCD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19721/19721 [01:00<00:00, 328.04it/s]\n",
      "100%|██████████| 19721/19721 [00:59<00:00, 332.11it/s]\n",
      "100%|██████████| 19721/19721 [04:36<00:00, 71.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating: NVDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17070/17070 [01:38<00:00, 172.88it/s]\n",
      "100%|██████████| 17070/17070 [01:28<00:00, 193.63it/s]\n",
      "100%|██████████| 17070/17070 [03:56<00:00, 72.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating: TSLA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73272/73272 [03:29<00:00, 349.47it/s] \n",
      "100%|██████████| 73272/73272 [03:25<00:00, 357.19it/s] \n",
      "100%|██████████| 73272/73272 [17:42<00:00, 68.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating: NFLX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10263/10263 [01:06<00:00, 155.11it/s]\n",
      "100%|██████████| 10263/10263 [01:00<00:00, 170.09it/s]\n",
      "100%|██████████| 10263/10263 [02:27<00:00, 69.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating: GME\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 570328/570328 [13:14<00:00, 717.66it/s] \n",
      "100%|██████████| 570328/570328 [13:41<00:00, 694.51it/s] \n",
      "100%|██████████| 570328/570328 [2:23:57<00:00, 66.03it/s]  \n"
     ]
    }
   ],
   "source": [
    "for f in files:\n",
    "    symbol = f.split('.')[0]\n",
    "    logging.basicConfig(filename=f'log/{symbol}.log', level=logging.INFO)\n",
    "    df = pd.read_csv(f'prepared/{symbol}.csv').drop(['stock_symbol'], axis=1)\n",
    "    \n",
    "    print(f'Calculating: {symbol.upper()}')\n",
    "    \n",
    "    # Vader\n",
    "    counter = 0\n",
    "    total = len(df['combined'])\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    df['polarity_vader'] = df['combined'].progress_apply(get_sentiment_vader)\n",
    "    \n",
    "    # Pattern\n",
    "    counter = 0\n",
    "    df['pattern_temp'] = df['combined'].progress_apply(get_sentiment_pattern)\n",
    "    df[['polarity_pattern', 'subjectivity_pattern']] = df['pattern_temp'].apply(pd.Series)\n",
    "    df = df.drop(['pattern_temp'], axis=1)\n",
    "    \n",
    "    # XLM-T\n",
    "    counter = 0\n",
    "    df['polarity_xlm-t'] = df['combined'].progress_apply(get_sentiment_xlm)\n",
    "    \n",
    "    df.to_csv(f'sentiment/{symbol}_sentiment.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
